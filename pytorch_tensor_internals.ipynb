{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNHJMqTJKIBA5Imr9M5T1+x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ksharat45/Pytorch/blob/main/pytorch_tensor_internals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqGF-LHr3vt0",
        "outputId": "f16b4d08-e364-42ef-9652-5c329ba1d522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tensor: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "Original Data Pointer: 756042048\n",
            "---\n",
            "## Case A: view() (Always creates a View if Contiguous) ##\n",
            "View Tensor: tensor([[0, 1, 2, 3, 4],\n",
            "        [5, 6, 7, 8, 9]])\n",
            "View Data Pointer: 756042048\n",
            "Memory Shared (view)? True (It's a VIEW)\n",
            "--------------------\n",
            "## Case B: reshape() (Creates a View when Contiguous) ##\n",
            "Reshape (View) Tensor: tensor([[0, 1],\n",
            "        [2, 3],\n",
            "        [4, 5],\n",
            "        [6, 7],\n",
            "        [8, 9]])\n",
            "Reshape (View) Data Pointer: 756042048\n",
            "Memory Shared (reshape/view)? True (It's a VIEW)\n",
            "--------------------\n",
            "## Case C: reshape() (Creates a Copy when Non-Contiguous) ##\n",
            "non_contiguous_tensor(view): tensor([[0, 5],\n",
            "        [1, 6],\n",
            "        [2, 7],\n",
            "        [3, 8],\n",
            "        [4, 9]])\n",
            "Non-Contiguous Data Pointer: 668047104\n",
            "Reshape (Copy) Tensor: tensor([0, 5, 1, 6, 2, 7, 3, 8, 4, 9])\n",
            "Reshape (Copy) Data Pointer: 767621952\n",
            "Memory Shared (reshape/copy)? False (It's a COPY)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 1. Original Contiguous Tensor\n",
        "original_tensor = torch.arange(10)\n",
        "print(f\"Original Tensor: {original_tensor}\")\n",
        "print(f\"Original Data Pointer: {original_tensor.data_ptr()}\")\n",
        "print(\"---\")\n",
        "\n",
        "## Case A: Using view() on a Contiguous Tensor\n",
        "print(\"## Case A: view() (Always creates a View if Contiguous) ##\")\n",
        "view_tensor = original_tensor.view(2, 5)\n",
        "print(f\"View Tensor: {view_tensor}\")\n",
        "print(f\"View Data Pointer: {view_tensor.data_ptr()}\")\n",
        "\n",
        "# Check for shared memory\n",
        "is_view_shared = original_tensor.data_ptr() == view_tensor.data_ptr()\n",
        "print(f\"Memory Shared (view)? {is_view_shared} (It's a VIEW)\")\n",
        "print(\"--------------------\")\n",
        "\n",
        "## Case B: Using reshape() on a Contiguous Tensor\n",
        "print(\"## Case B: reshape() (Creates a View when Contiguous) ##\")\n",
        "reshape_view_tensor = original_tensor.reshape(5, 2)\n",
        "print(f\"Reshape (View) Tensor: {reshape_view_tensor}\")\n",
        "print(f\"Reshape (View) Data Pointer: {reshape_view_tensor.data_ptr()}\")\n",
        "\n",
        "# Check for shared memory\n",
        "is_reshape_view_shared = original_tensor.data_ptr() == reshape_view_tensor.data_ptr()\n",
        "print(f\"Memory Shared (reshape/view)? {is_reshape_view_shared} (It's a VIEW)\")\n",
        "print(\"--------------------\")\n",
        "\n",
        "## Case C: Forcing reshape() to create a Copy\n",
        "print(\"## Case C: reshape() (Creates a Copy when Non-Contiguous) ##\")\n",
        "# 1. Create a NON-CONTIGUOUS tensor using transpose()\n",
        "non_contiguous_tensor = torch.arange(10).reshape(2, 5).T\n",
        "print(f\"non_contiguous_tensor(view): {non_contiguous_tensor}\")\n",
        "print(f\"Non-Contiguous Data Pointer: {non_contiguous_tensor.data_ptr()}\")\n",
        "# Note: Calling .view() on this tensor would raise an error!\n",
        "\n",
        "# 2. Call reshape() on the non-contiguous tensor\n",
        "reshape_copy_tensor = non_contiguous_tensor.reshape(10)\n",
        "print(f\"Reshape (Copy) Tensor: {reshape_copy_tensor}\")\n",
        "print(f\"Reshape (Copy) Data Pointer: {reshape_copy_tensor.data_ptr()}\")\n",
        "\n",
        "# Check for shared memory\n",
        "is_reshape_copy_shared = non_contiguous_tensor.data_ptr() == reshape_copy_tensor.data_ptr()\n",
        "print(f\"Memory Shared (reshape/copy)? {is_reshape_copy_shared} (It's a COPY)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n========== STEP 1: CREATE CONTIGUOUS TENSOR x ==========\\n\")\n",
        "\n",
        "# Create tensor x\n",
        "x = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "\n",
        "# Metadata of x\n",
        "print(\"Tensor x metadata:\")\n",
        "print(\"shape  :\", x.shape)\n",
        "print(\"stride :\", x.stride())\n",
        "print(\"is_contiguous:\", x.is_contiguous())\n",
        "\n",
        "# Memory view of x\n",
        "print(\"\\nMemory storage of x:\")\n",
        "print(list(x.storage()))\n",
        "\n",
        "# Print x\n",
        "print(\"\\nTensor x:\")\n",
        "print(x)\n",
        "\n",
        "\n",
        "print(\"\\n========== STEP 2: TRANSPOSE → CREATE y ==========\\n\")\n",
        "\n",
        "# Transpose\n",
        "y = x.t()\n",
        "\n",
        "# Orientation change\n",
        "print(\"Tensor y = x.t() (transpose):\")\n",
        "print(y)\n",
        "\n",
        "print(\"\\nNotice how orientation changes (rows ↔ columns)\")\n",
        "print(\"Elements appear shuffled visually, but memory is unchanged.\")\n",
        "\n",
        "# Metadata of y\n",
        "print(\"\\nTensor y metadata:\")\n",
        "print(\"shape  :\", y.shape)\n",
        "print(\"stride :\", y.stride())\n",
        "print(\"is_contiguous:\", y.is_contiguous())\n",
        "\n",
        "# Memory view of y\n",
        "print(\"\\nMemory storage of y (same as x):\")\n",
        "print(list(y.storage()))\n",
        "\n",
        "\n",
        "print(\"\\n========== STEP 3: ROW-MAJOR LOGICAL ACCESS OF y ==========\\n\")\n",
        "\n",
        "print(\"Row-major traversal of y (logical order):\")\n",
        "\n",
        "logical_order = []\n",
        "for i in range(y.shape[0]):\n",
        "    for j in range(y.shape[1]):\n",
        "        val = y[i, j].item()\n",
        "        logical_order.append(val)\n",
        "        print(f\"y[{i},{j}] = {val}\")\n",
        "\n",
        "print(\"\\nLogical row-major order of y:\")\n",
        "print(logical_order)\n",
        "\n",
        "\n",
        "print(\"\\n========== STEP 4: MEMORY ORDER vs LOGICAL ORDER ==========\\n\")\n",
        "\n",
        "memory_order = list(y.storage())\n",
        "\n",
        "print(\"Memory order (actual storage):\")\n",
        "print(memory_order)\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "print(\"Logical row-major order :\", logical_order)\n",
        "print(\"Actual memory order     :\", memory_order)\n",
        "\n",
        "\n",
        "print(\"\\n========== STEP 5: CONTIGUITY CHECK ==========\\n\")\n",
        "\n",
        "if not y.is_contiguous():\n",
        "    print(\"❌ y is NOT contiguous\")\n",
        "    print(\"Reason: logical row-major order ≠ memory order\")\n",
        "else:\n",
        "    print(\"✅ y is contiguous\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksAtir2Q39Pf",
        "outputId": "513c5985-4864-4d03-923f-88c73180a3b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== STEP 1: CREATE CONTIGUOUS TENSOR x ==========\n",
            "\n",
            "Tensor x metadata:\n",
            "shape  : torch.Size([2, 3])\n",
            "stride : (3, 1)\n",
            "is_contiguous: True\n",
            "\n",
            "Memory storage of x:\n",
            "[1, 2, 3, 4, 5, 6]\n",
            "\n",
            "Tensor x:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "\n",
            "========== STEP 2: TRANSPOSE → CREATE y ==========\n",
            "\n",
            "Tensor y = x.t() (transpose):\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n",
            "\n",
            "Notice how orientation changes (rows ↔ columns)\n",
            "Elements appear shuffled visually, but memory is unchanged.\n",
            "\n",
            "Tensor y metadata:\n",
            "shape  : torch.Size([3, 2])\n",
            "stride : (1, 3)\n",
            "is_contiguous: False\n",
            "\n",
            "Memory storage of y (same as x):\n",
            "[1, 2, 3, 4, 5, 6]\n",
            "\n",
            "========== STEP 3: ROW-MAJOR LOGICAL ACCESS OF y ==========\n",
            "\n",
            "Row-major traversal of y (logical order):\n",
            "y[0,0] = 1\n",
            "y[0,1] = 4\n",
            "y[1,0] = 2\n",
            "y[1,1] = 5\n",
            "y[2,0] = 3\n",
            "y[2,1] = 6\n",
            "\n",
            "Logical row-major order of y:\n",
            "[1, 4, 2, 5, 3, 6]\n",
            "\n",
            "========== STEP 4: MEMORY ORDER vs LOGICAL ORDER ==========\n",
            "\n",
            "Memory order (actual storage):\n",
            "[1, 2, 3, 4, 5, 6]\n",
            "\n",
            "Comparison:\n",
            "Logical row-major order : [1, 4, 2, 5, 3, 6]\n",
            "Actual memory order     : [1, 2, 3, 4, 5, 6]\n",
            "\n",
            "========== STEP 5: CONTIGUITY CHECK ==========\n",
            "\n",
            "❌ y is NOT contiguous\n",
            "Reason: logical row-major order ≠ memory order\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1866011434.py:15: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  print(list(x.storage()))\n"
          ]
        }
      ]
    }
  ]
}