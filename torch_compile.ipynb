{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAzwHkkY//CygX//6ReaDu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ksharat45/Pytorch/blob/main/torch_compile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J0kqn0kYWrMO"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "8IOO8DkbXJnJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "    def forward(self, x):\n",
        "        print(\"➡️ Python forward executing\")\n",
        "        y = x * 2\n",
        "        z = torch.relu(y)\n",
        "        return z.sum()\n",
        "\n",
        "model = MyModel()\n",
        "x = torch.randn(4, 4)\n",
        "\n",
        "print(\"Eager output:\", model(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XzHVCScYEhD",
        "outputId": "85c37c35-a411-4734-c570-91bbc0cdccbe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "➡️ Python forward executing\n",
            "Eager output: tensor(13.7301)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch._dynamo as dynamo\n",
        "\n",
        "dynamo.config.verbose = True\n",
        "dynamo.config.suppress_errors = False"
      ],
      "metadata": {
        "id": "rUYD72TpYmYt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compiled_model = torch.compile(\n",
        "    model,\n",
        "    backend=\"inductor\",   # TorchInductor\n",
        "    fullgraph=False       # allow graph breaks\n",
        ")"
      ],
      "metadata": {
        "id": "NhglumjcYsl4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- First compiled run ---\")\n",
        "out = compiled_model(x)\n",
        "print(\"Compiled output:\", out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rz6dpb0Yt6J",
        "outputId": "f4d68cf9-b5f8-44db-bec3-5476a42c41b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- First compiled run ---\n",
            "➡️ Python forward executing\n",
            "Compiled output: tensor(13.7301)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._dynamo import explain\n",
        "\n",
        "explanation = explain(model, x)\n",
        "print(explanation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VqOaEf4Y5K7",
        "outputId": "68038bf7-add9-4361-afe6-c4d94bc60427"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "➡️ Python forward executing\n",
            "Graph Count: 1\n",
            "Graph Break Count: 0\n",
            "Op Count: 2\n",
            "Break Reasons:\n",
            "Ops per Graph:\n",
            "  Ops 1:\n",
            "    <built-in function mul>\n",
            "    <built-in method relu of type object at 0x7f02ef3828c0>\n",
            "Out Guards:\n",
            "  Guard 1:\n",
            "    Name: ''\n",
            "    Source: global\n",
            "    Create Function: AUTOGRAD_SAVED_TENSORS_HOOKS\n",
            "    Guard Types: ['AUTOGRAD_SAVED_TENSORS_HOOKS']\n",
            "    Code List: ['torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None']\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "  Guard 2:\n",
            "    Name: ''\n",
            "    Source: global\n",
            "    Create Function: GRAD_MODE\n",
            "    Guard Types: None\n",
            "    Code List: None\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "  Guard 3:\n",
            "    Name: \"L['x']\"\n",
            "    Source: local\n",
            "    Create Function: TENSOR_MATCH\n",
            "    Guard Types: ['TENSOR_MATCH']\n",
            "    Code List: [\"hasattr(L['x'], '_dynamo_dynamic_indices') == False\"]\n",
            "    Object Weakref: <weakref at 0x7f02cd8cf0b0; to 'Tensor' at 0x7f02d2e6d270>\n",
            "    Guarded Class Weakref: <weakref at 0x7f03041c54e0; to 'torch._C._TensorMeta' at 0x10606c10 (Tensor)>\n",
            "  Guard 4:\n",
            "    Name: ''\n",
            "    Source: global\n",
            "    Create Function: DETERMINISTIC_ALGORITHMS\n",
            "    Guard Types: None\n",
            "    Code List: None\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "  Guard 5:\n",
            "    Name: \"G['torch'].relu\"\n",
            "    Source: global\n",
            "    Create Function: FUNCTION_MATCH\n",
            "    Guard Types: ['ID_MATCH']\n",
            "    Code List: [\"___check_obj_id(G['torch'].relu, 139650930998336)\"]\n",
            "    Object Weakref: <weakref at 0x7f02cca273d0; to 'builtin_function_or_method' at 0x7f0304227c40>\n",
            "    Guarded Class Weakref: <weakref at 0x7f0311c4c4a0; to 'type' at 0xa15ee0 (builtin_function_or_method)>\n",
            "  Guard 6:\n",
            "    Name: ''\n",
            "    Source: global\n",
            "    Create Function: TORCH_FUNCTION_STATE\n",
            "    Guard Types: None\n",
            "    Code List: None\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "  Guard 7:\n",
            "    Name: ''\n",
            "    Source: global\n",
            "    Create Function: DEFAULT_DEVICE\n",
            "    Guard Types: ['DEFAULT_DEVICE']\n",
            "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "  Guard 8:\n",
            "    Name: ''\n",
            "    Source: shape_env\n",
            "    Create Function: SHAPE_ENV\n",
            "    Guard Types: None\n",
            "    Code List: None\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "  Guard 9:\n",
            "    Name: \"G['torch']\"\n",
            "    Source: global\n",
            "    Create Function: FUNCTION_MATCH\n",
            "    Guard Types: ['ID_MATCH']\n",
            "    Code List: [\"___check_obj_id(G['torch'], 139650933024112)\"]\n",
            "    Object Weakref: <weakref at 0x7f02ccadd1c0; to 'module' at 0x7f0304416570>\n",
            "    Guarded Class Weakref: <weakref at 0x7f0311c7f010; to 'type' at 0xa15a00 (module)>\n",
            "Compile Times: TorchDynamo compilation metrics:\n",
            "Function                        Runtimes (s)\n",
            "------------------------------  ----------------------\n",
            "_compile.compile_inner          0.0338, 0.0369\n",
            "compile_attempt_0               0.0030, 0.0114\n",
            "bytecode_tracing                0.0013, 0.0002, 0.0054\n",
            "compile_attempt_1               0.0052\n",
            "build_guards                    0.0236, 0.0236\n",
            "gc                              0.0005, 0.0005\n",
            "OutputGraph.call_user_compiler  0.0002\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.12/unittest/mock.py:1396: FutureWarning: explain(f, *args, **kwargs) is deprecated, use explain(f)(*args, **kwargs) instead.  If you don't migrate, we may break your explain call in the future if your user defined kwargs conflict with future kwargs added to explain(f).\n",
            "  return func(*newargs, **newkeywargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.fx as fx\n",
        "\n",
        "graph = fx.symbolic_trace(model)\n",
        "print(\"\\nFX Graph:\")\n",
        "print(graph.graph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kc6KqFyZGGE",
        "outputId": "c61e342a-e822-4850-9ef6-9642dfb353d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "➡️ Python forward executing\n",
            "\n",
            "FX Graph:\n",
            "graph():\n",
            "    %x : [num_users=1] = placeholder[target=x]\n",
            "    %mul : [num_users=1] = call_function[target=operator.mul](args = (%x, 2), kwargs = {})\n",
            "    %relu : [num_users=1] = call_function[target=torch.relu](args = (%mul,), kwargs = {})\n",
            "    %sum_1 : [num_users=1] = call_method[target=sum](args = (%relu,), kwargs = {})\n",
            "    return sum_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Modify model:\n",
        "\n",
        "class BreakModel(nn.Module):\n",
        "    def forward(self, x):\n",
        "        print(\"Python print → graph break\")\n",
        "        if x.sum().item() > 0:\n",
        "            x = x * 2\n",
        "        return x.relu().sum()\n",
        "\n",
        "bm = BreakModel()\n",
        "compiled_bm = torch.compile(bm)\n",
        "\n",
        "compiled_bm(torch.randn(4,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YX_0edsZv5q",
        "outputId": "81c9d004-10fb-4539-f6ff-001e3497e837"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W1222 09:32:29.530000 169 torch/_dynamo/variables/tensor.py:1048] [1/0] Graph break from `Tensor.item()`, consider setting:\n",
            "W1222 09:32:29.530000 169 torch/_dynamo/variables/tensor.py:1048] [1/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
            "W1222 09:32:29.530000 169 torch/_dynamo/variables/tensor.py:1048] [1/0] or:\n",
            "W1222 09:32:29.530000 169 torch/_dynamo/variables/tensor.py:1048] [1/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
            "W1222 09:32:29.530000 169 torch/_dynamo/variables/tensor.py:1048] [1/0] to include these operations in the captured graph.\n",
            "W1222 09:32:29.530000 169 torch/_dynamo/variables/tensor.py:1048] [1/0] \n",
            "W1222 09:32:29.530000 169 torch/_dynamo/variables/tensor.py:1048] [1/0] Graph break: from user code at:\n",
            "W1222 09:32:29.530000 169 torch/_dynamo/variables/tensor.py:1048] [1/0]   File \"/tmp/ipython-input-2944648084.py\", line 6, in torch_dynamo_resume_in_forward_at_5\n",
            "W1222 09:32:29.530000 169 torch/_dynamo/variables/tensor.py:1048] [1/0]     if x.sum().item() > 0:\n",
            "W1222 09:32:29.530000 169 torch/_dynamo/variables/tensor.py:1048] [1/0] \n",
            "W1222 09:32:29.530000 169 torch/_dynamo/variables/tensor.py:1048] [1/0] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python print → graph break\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.3360)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compiled_strict = torch.compile(\n",
        "    bm,\n",
        "    fullgraph=True\n",
        ")\n",
        "\n",
        "compiled_strict(torch.randn(4,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "metwGN7tZ3t0",
        "outputId": "d9095fed-ebe9-4be0-f82d-f7ccaa681627"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python print → graph break\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(20.4656)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.environ.get(\"TORCHINDUCTOR_CACHE_DIR\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDzay2chZ8E8",
        "outputId": "c5802d97-50b9-4c52-c795-131f7a858700"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/torchinductor_root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch._inductor.config.debug = True"
      ],
      "metadata": {
        "id": "UW9nx6dTZ9ML"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compiled_bm(torch.randn(4,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzbQ58ypaLdt",
        "outputId": "cd3f3d0d-096c-4023-ef8f-ec1b84e65f6c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python print → graph break\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(13.5818)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}